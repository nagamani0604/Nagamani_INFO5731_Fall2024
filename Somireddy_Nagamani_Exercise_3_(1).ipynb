{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagamani0604/Nagamani_INFO5731_Fall2024/blob/main/Somireddy_Nagamani_Exercise_3_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "An interesting text classification task is topic categorization for news articles. Sorting news articles into pre-established categories—such as politics, sports, entertainment, and technology—is the goal. The following five feature types can be used in order to create a machine learning model for this task:\n",
        "\n",
        "\n",
        "1. Bag-of-Words (BoW) Features\n",
        "A simple example in which every article is defined by the frequency or absence of specific words.\n",
        "Words like \"game\" or \"team\" in sports, or \"government\" or \"election\" in politics, have a tendency to have a strong connection with particular subjects. The topic can be strongly derived from the frequency of particular words, even in the absence of an understanding of word order.\n",
        "\n",
        "2. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "A weighted version of word counts that includes word frequency in each article.\n",
        "Common words like \"the\" and \"is\" are given less weight by TF-IDF, which also highlights topic-specific words that are more informative but occur less frequently.\n",
        "\n",
        "3. Named Entity Recognition (NER)\n",
        "Recognizing and classifying named entities in the text, such as individuals, groups, dates, and places.\n",
        "There are some things that are closely related to particular subjects. Refers to athletes point to a sports article, whereas mentions of politicians or political organizations most likely indicate a politics article.\n",
        "\n",
        "4. Document Length\n",
        "The number of words in an article.\n",
        "In general, different subjects are covered in different lengths. For example, sports articles are typically longer and include brief event summaries, whereas business or politics articles may offer more in-depth analysis. The length of a document can be a basic but useful feature.\n",
        "\n",
        "5. Topic-Specific Keywords\n",
        "Utilizing keyword extraction algorithms or past knowledge to identify words or phrases that serve as important indicators of particular categories.\n",
        "In sports, terms like \"match,\" and \"score\" are highly representative of the subject matter; in politics, terms like \"policy,\" and \"election\" are similarly representative. These keywords offer clear hints regarding the article's subject.\n",
        "When these features are combined, the articles are extensively represented, which improves the model's comprehension of the context and content for topic categorization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "665815e7-20cd-4033-cbee-180e1aee36e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Bag-of-Words Feature Matrix (BoW):\n",
            " [[0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 3\n",
            "  1 0 0 0]\n",
            " [0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0\n",
            "  0 0 1 0]\n",
            " [1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0\n",
            "  1 0 0 1]\n",
            " [0 0 0 1 0 2 1 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 3\n",
            "  0 1 0 0]]\n",
            "BoW Feature Names: ['advance' 'after' 'ai' 'and' 'announced' 'are' 'around' 'athletes'\n",
            " 'becoming' 'boost' 'companies' 'competition' 'corner' 'economy'\n",
            " 'election' 'energy' 'for' 'games' 'gearing' 'government' 'like'\n",
            " 'microsoft' 'more' 'new' 'olympic' 'other' 'partnership' 'policies'\n",
            " 'popular' 'power' 'renewable' 'research' 'solar' 'sources' 'tech' 'the'\n",
            " 'to' 'up' 'wind' 'with']\n",
            "\n",
            "TF-IDF Feature Matrix:\n",
            " [[0.         0.26882576 0.         0.         0.21194532 0.\n",
            "  0.         0.         0.         0.26882576 0.         0.\n",
            "  0.         0.26882576 0.26882576 0.         0.         0.\n",
            "  0.         0.26882576 0.         0.         0.         0.26882576\n",
            "  0.         0.         0.         0.26882576 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.63583595\n",
            "  0.21194532 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.23513012 0.         0.23513012\n",
            "  0.         0.         0.29823274 0.         0.         0.\n",
            "  0.         0.         0.         0.29823274 0.         0.\n",
            "  0.         0.         0.29823274 0.         0.29823274 0.\n",
            "  0.         0.         0.         0.         0.29823274 0.29823274\n",
            "  0.29823274 0.         0.29823274 0.29823274 0.         0.\n",
            "  0.         0.         0.29823274 0.        ]\n",
            " [0.31245141 0.         0.31245141 0.         0.24634028 0.\n",
            "  0.         0.         0.         0.         0.31245141 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.31245141 0.         0.\n",
            "  0.         0.31245141 0.31245141 0.         0.         0.\n",
            "  0.         0.31245141 0.         0.         0.31245141 0.\n",
            "  0.24634028 0.         0.         0.31245141]\n",
            " [0.         0.         0.         0.18738643 0.         0.37477285\n",
            "  0.23767592 0.23767592 0.         0.         0.         0.23767592\n",
            "  0.23767592 0.         0.         0.         0.23767592 0.23767592\n",
            "  0.23767592 0.         0.         0.         0.         0.\n",
            "  0.23767592 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.56215928\n",
            "  0.         0.23767592 0.         0.        ]]\n",
            "TF-IDF Feature Names: ['advance' 'after' 'ai' 'and' 'announced' 'are' 'around' 'athletes'\n",
            " 'becoming' 'boost' 'companies' 'competition' 'corner' 'economy'\n",
            " 'election' 'energy' 'for' 'games' 'gearing' 'government' 'like'\n",
            " 'microsoft' 'more' 'new' 'olympic' 'other' 'partnership' 'policies'\n",
            " 'popular' 'power' 'renewable' 'research' 'solar' 'sources' 'tech' 'the'\n",
            " 'to' 'up' 'wind' 'with']\n",
            "\n",
            "Named Entities (NER):\n",
            "Document 1: []\n",
            "Document 2: []\n",
            "Document 3: [('Microsoft', 'ORG'), ('AI', 'ORG')]\n",
            "Document 4: [('The Olympic Games', 'EVENT')]\n",
            "\n",
            "Document Lengths (in words): [12, 12, 12, 15]\n",
            "\n",
            "Topic-Specific Keywords:\n",
            "Document 1: {'politics': ['government']}\n",
            "Document 2: {'environment': ['energy', 'solar', 'wind', 'power']}\n",
            "Document 3: {'technology': ['Microsoft', 'AI', 'tech']}\n",
            "Document 4: {'sports': ['Olympic', 'athletes']}\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn spacy\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "documents = [\n",
        "    \"The government announced new policies to boost the economy after the election.\",\n",
        "    \"Renewable energy sources like solar and wind power are becoming more popular.\",\n",
        "    \"Microsoft announced a partnership with other tech companies to advance AI research.\",\n",
        "    \"The Olympic Games are around the corner, and athletes are gearing up for the competition.\"\n",
        "]\n",
        "\n",
        "# 1. Bag-of-Words (BoW) Features\n",
        "vectorizer_bow = CountVectorizer()\n",
        "bow_matrix = vectorizer_bow.fit_transform(documents)\n",
        "\n",
        "print(\"Bag-of-Words Feature Matrix (BoW):\\n\", bow_matrix.toarray())\n",
        "print(\"BoW Feature Names:\", vectorizer_bow.get_feature_names_out())\n",
        "\n",
        "# 2. TF-IDF (Term Frequency-Inverse Document Frequency) Features\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer_tfidf.fit_transform(documents)\n",
        "\n",
        "print(\"\\nTF-IDF Feature Matrix:\\n\", tfidf_matrix.toarray())\n",
        "print(\"TF-IDF Feature Names:\", vectorizer_tfidf.get_feature_names_out())\n",
        "\n",
        "# 3. Named Entity Recognition (NER) Features\n",
        "def extract_named_entities(doc):\n",
        "    doc_nlp = nlp(doc)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc_nlp.ents]\n",
        "    return entities\n",
        "\n",
        "named_entities = [extract_named_entities(doc) for doc in documents]\n",
        "print(\"\\nNamed Entities (NER):\")\n",
        "for i, entities in enumerate(named_entities):\n",
        "    print(f\"Document {i+1}: {entities}\")\n",
        "\n",
        "# 4. Document Length\n",
        "doc_lengths = [len(doc.split()) for doc in documents]\n",
        "print(\"\\nDocument Lengths (in words):\", doc_lengths)\n",
        "\n",
        "# 5. Topic-Specific Keywords\n",
        "keywords = {\n",
        "    \"politics\": [\"government\", \"election\", \"policy\"],\n",
        "    \"sports\": [\"Olympic\", \"athletes\", \"competition\", \"game\"],\n",
        "    \"technology\": [\"Microsoft\", \"AI\", \"tech\", \"research\"],\n",
        "    \"environment\": [\"energy\", \"solar\", \"wind\", \"power\"]\n",
        "}\n",
        "\n",
        "def extract_keywords(doc, keywords):\n",
        "    doc_tokens = doc.split()\n",
        "    found_keywords = {}\n",
        "    for category, kw_list in keywords.items():\n",
        "        found = [kw for kw in kw_list if kw in doc_tokens]\n",
        "        if found:\n",
        "            found_keywords[category] = found\n",
        "    return found_keywords\n",
        "\n",
        "doc_keywords = [extract_keywords(doc, keywords) for doc in documents]\n",
        "print(\"\\nTopic-Specific Keywords:\")\n",
        "for i, kw in enumerate(doc_keywords):\n",
        "    print(f\"Document {i+1}: {kw}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "116505bf-673e-441e-b3ee-9d152f7c98c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Features that are based on Chi-Square Test:\n",
            "          feature  chi2_score\n",
            "15         pandas    0.740849\n",
            "4          easier    0.740849\n",
            "13   manipulation    0.740849\n",
            "0              ai    0.670820\n",
            "7          future    0.670820\n",
            "14            our    0.670820\n",
            "8         impacts    0.670820\n",
            "17  understanding    0.670820\n",
            "1        analysis    0.597156\n",
            "16         python    0.573138\n",
            "6     fascinating    0.378676\n",
            "10             is    0.378676\n",
            "5            easy    0.361484\n",
            "11           love    0.348906\n",
            "2          coding    0.348906\n",
            "9              in    0.348906\n",
            "3            data    0.037978\n",
            "12          makes    0.020479\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "# Required libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = {'text': ['I love coding in Python', 'Data analysis is fascinating', 'Python makes data analysis easy',\n",
        "                'Pandas makes data manipulation easier', 'Understanding AI impacts our future'],\n",
        "        'label': [1, 1, 1, 0, 0]}  # 1 = Positive, 0 = Negative\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract Features using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X = tfidf_vectorizer.fit_transform(df['text'])  # Feature matrix\n",
        "y = df['label']  # Labels\n",
        "\n",
        "# Apply Chi-Square test\n",
        "chi2_scores, p_values = chi2(X, y)\n",
        "\n",
        "# Create a DataFrame with feature names and their corresponding Chi-Square scores\n",
        "feature_ranking = pd.DataFrame({'feature': tfidf_vectorizer.get_feature_names_out(), 'chi2_score': chi2_scores})\n",
        "\n",
        "# Rank features based on Chi-Square scores in descending order\n",
        "ranked_features = feature_ranking.sort_values(by='chi2_score', ascending=False)\n",
        "\n",
        "print(\"Ranked Features that are based on Chi-Square Test:\")\n",
        "print(ranked_features)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a016d51b-82c0-472d-f533-49ca350904e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Documents based on Query Similarity:\n",
            "\n",
            "Document: The government announced new policies to boost the economy after the election.\n",
            "Similarity Score: 0.8176\n",
            "\n",
            "Document: Renewable energy sources like solar and wind power are becoming more popular.\n",
            "Similarity Score: 0.7989\n",
            "\n",
            "Document: The Olympic Games are around the corner, and athletes are gearing up for the competition.\n",
            "Similarity Score: 0.7886\n",
            "\n",
            "Document: Microsoft announced a partnership with other tech companies to advance AI research.\n",
            "Similarity Score: 0.7816\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch scikit-learn\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "documents = [\n",
        "    \"The government announced new policies to boost the economy after the election.\",\n",
        "    \"Renewable energy sources like solar and wind power are becoming more popular.\",\n",
        "    \"Microsoft announced a partnership with other tech companies to advance AI research.\",\n",
        "    \"The Olympic Games are around the corner, and athletes are gearing up for the competition.\"\n",
        "]\n",
        "\n",
        "query = \"Government policies to boost the economy\"\n",
        "\n",
        "def get_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "    return cls_embedding\n",
        "\n",
        "query_embedding = get_bert_embedding(query)\n",
        "\n",
        "document_embeddings = np.vstack([get_bert_embedding(doc) for doc in documents])\n",
        "\n",
        "# Calculate cosine similarity between query and each document\n",
        "similarities = cosine_similarity(query_embedding, document_embeddings)[0]\n",
        "\n",
        "# Rank documents based on similarity in descending order\n",
        "ranked_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "print(\"Ranked Documents based on Query Similarity:\\n\")\n",
        "for idx in ranked_indices:\n",
        "    print(f\"Document: {documents[idx]}\\nSimilarity Score: {similarities[idx]:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "I acquired valuable experience and a deeper understanding of different Natural Language Processing (NLP) techniques working on feature extraction from text data.\n",
        "Key ideas like Named Entity Recognition (NER), Term Frequency-Inverse Document Frequency (TF-IDF), Bag-of-Words (BoW), and the usage of pre-trained models like BERT made it easier for me to understand the various ways that textual data can be represented for machine learning tasks.\n",
        "\n",
        "Understanding how various feature extraction techniques differ in complexity and applicability was one of the major challenges.\n",
        "For example, BERT embeddings require more computational power and knowledge of deep learning models, while Bag-of-Words and TF-IDF are simple but less successful in capturing word meanings.\n",
        "\n",
        "This exercise has a lot to do with natural language processing (NLP), which deals with converting unformatted text into formats that computers can understand.\n",
        "Text classification, sentiment analysis, information retrieval, and document ranking are just a few of the NLP tasks that heavily rely on feature extraction techniques like BoW, TF-IDF, and BERT.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}