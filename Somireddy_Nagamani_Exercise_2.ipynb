{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagamani0604/Nagamani_INFO5731_Fall2024/blob/main/Somireddy_Nagamani_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "\n",
        "How can adults in high-pressure jobs reduce stress and improve the quality of their sleep with regular mindfulness meditation?\n",
        "\n",
        "Data to be collected:\n",
        "1. Information on Mindfulness Meditation:\n",
        "Duration,consistency,type and time : Lenght of the time, frequency of practice, What type of meditation at what time during the day is the basic data that needs to be collected\n",
        "2. Sleep quality: Sleep duration and efficiency\n",
        "3. Stress Level: Cortisol levels and heart rate variability\n",
        "4. Demographic data: Age, gender, job type, job role and work hours\n",
        "\n",
        "Amount of data needed for analysis:\n",
        "1. There should be atleast 200 participants from high pressure jobs and even more for reliable results\n",
        "2. All the particiapnts should follow mindfulness meditation atleast for 4-5 weeks to observe any changes in the stress levels and sleep patterns\n",
        "3. Stress levels and sleep quality should be measured before starting the assessment, mid-assessment and at the end of the study.\n",
        "\n",
        "Steps for collecting and saving the data:\n",
        "1. We need to recruit participants who are working in high stressful jobs like IT employees or health care workers who are interested in mindfulness meditation\n",
        "2. All the participants should have access to the mindfulness medidation apps. We need to ensure the participants log in into their apps and track the duration and frequency\n",
        "3. Track the sleep patterns like total sleep time, efficiency and awakenings\n",
        "4. Measure the stress levels by instructing the participants to wear devices that track heart rate variability\n",
        "5. Save all the collected data in a database.\n",
        "6. Compare pre and post meditation data and analyse the improvements in sleep quality and stress reduction.\n",
        "7. Continuosly follow up and montor the participants medidation schedule to ensure the accurate data is collected.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "e6edfd44-2997-447f-d7b1-85872a14a1fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-0a8fe7ac9284>, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0a8fe7ac9284>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    Data to be collected:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Simpler functions to generate random data\n",
        "def generate_duration():\n",
        "    return random.choice([20, 40, 60])\n",
        "\n",
        "def generate_sleep_quality():\n",
        "    return random.randint(10, 10)\n",
        "\n",
        "def generate_stress_level():\n",
        "    return random.randint(1, 10)\n",
        "\n",
        "# Generate 1000 samples\n",
        "data = []\n",
        "for i in range(1000):\n",
        "    duration = generate_duration()\n",
        "    sleep_quality = generate_sleep_quality()\n",
        "    stress_level = generate_stress_level()\n",
        "\n",
        "    # Append data to list\n",
        "    data.append([duration, sleep_quality, stress_level])\n",
        "\n",
        "# Convert the data to a DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"Duration\", \"Sleep_Quality\", \"Stress_Level\"])\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Save the data to a CSV file\n",
        "df.to_csv(\"simple_meditation_dataset.csv\", index=False)\n",
        "\n",
        "print(\"Dataset generated and saved as 'simple_meditation_dataset.csv'.\")\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd93c21e-d305-4584-9fb9-5f55d21d4edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Duration  Sleep_Quality  Stress_Level\n",
            "0        20             10             9\n",
            "1        40             10             3\n",
            "2        60             10             6\n",
            "3        20             10             9\n",
            "4        20             10             9\n",
            "Dataset generated and saved as 'simple_meditation_dataset.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "565b03df-77de-42a8-bfcf-a4f8561b93f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 20/1000 articles.\n",
            "Collected 40/1000 articles.\n",
            "Collected 60/1000 articles.\n",
            "Collected 80/1000 articles.\n",
            "Collected 100/1000 articles.\n",
            "Collected 120/1000 articles.\n",
            "Collected 140/1000 articles.\n",
            "Collected 160/1000 articles.\n",
            "Collected 180/1000 articles.\n",
            "Collected 200/1000 articles.\n",
            "Collected 220/1000 articles.\n",
            "Collected 240/1000 articles.\n",
            "Collected 260/1000 articles.\n",
            "Collected 280/1000 articles.\n",
            "Collected 300/1000 articles.\n",
            "Collected 320/1000 articles.\n",
            "Collected 340/1000 articles.\n",
            "Collected 360/1000 articles.\n",
            "Collected 380/1000 articles.\n",
            "Collected 400/1000 articles.\n",
            "Collected 420/1000 articles.\n",
            "Collected 440/1000 articles.\n",
            "Collected 460/1000 articles.\n",
            "Collected 480/1000 articles.\n",
            "Collected 500/1000 articles.\n",
            "Collected 520/1000 articles.\n",
            "Collected 540/1000 articles.\n",
            "Collected 560/1000 articles.\n",
            "Collected 580/1000 articles.\n",
            "Collected 600/1000 articles.\n",
            "Collected 620/1000 articles.\n",
            "Collected 640/1000 articles.\n",
            "Collected 660/1000 articles.\n",
            "Collected 680/1000 articles.\n",
            "Collected 700/1000 articles.\n",
            "Collected 720/1000 articles.\n",
            "Collected 740/1000 articles.\n",
            "Collected 760/1000 articles.\n",
            "Collected 780/1000 articles.\n",
            "Collected 800/1000 articles.\n",
            "Collected 820/1000 articles.\n",
            "Collected 840/1000 articles.\n",
            "Collected 860/1000 articles.\n",
            "Collected 880/1000 articles.\n",
            "Collected 900/1000 articles.\n",
            "Collected 920/1000 articles.\n",
            "Collected 940/1000 articles.\n",
            "Collected 960/1000 articles.\n",
            "Collected 980/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Collected 992/1000 articles.\n",
            "Error: 429\n",
            "Response content: {\n",
            "  \"error\": \"Your account has been throttled. You are exceeding 100 searches per hour. Please upgrade your plan, spread out your searches, or contact support.\"\n",
            "}\n",
            "Data collection completed. Articles saved in 'google_scholar_XYZ_articles.csv'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "SERPAPI_URL = 'https://serpapi.com/search'\n",
        "\n",
        "SERPAPI_KEY = '599b3eebc71b457caabdf91c7ecb6816f01eb9c63af91827b470dd6cc0fb8a78'\n",
        "\n",
        "params = {\n",
        "    'engine': 'google_scholar',\n",
        "    'q': 'XYZ',\n",
        "    'as_ylo': '2014',\n",
        "    'as_yhi': '2024',\n",
        "    'api_key': SERPAPI_KEY,\n",
        "    'num': 100\n",
        "}\n",
        "\n",
        "articles = []\n",
        "\n",
        "def get_scholar_data(params):\n",
        "    response = requests.get(SERPAPI_URL, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        print(f\"Response content: {response.text}\")\n",
        "        return None\n",
        "\n",
        "def collect_articles(max_articles=1000):\n",
        "    articles_collected = 0\n",
        "    start = 0\n",
        "\n",
        "    while articles_collected < max_articles:\n",
        "        params['start'] = start\n",
        "\n",
        "        data = get_scholar_data(params)\n",
        "        if not data:\n",
        "            break\n",
        "\n",
        "        for result in data.get('organic_results', []):\n",
        "            title = result.get('title', 'N/A')\n",
        "            year = result.get('publication_info', {}).get('year', 'N/A')\n",
        "            venue = result.get('publication_info', {}).get('journal', 'N/A')\n",
        "            authors = ', '.join(author.get('name') for author in result.get('authors', [])) if 'authors' in result else 'N/A'\n",
        "            abstract = result.get('snippet', 'N/A')\n",
        "\n",
        "            articles.append([title, venue, year, authors, abstract])\n",
        "            articles_collected += 1\n",
        "\n",
        "            if articles_collected >= max_articles:\n",
        "                break\n",
        "\n",
        "        start += 20\n",
        "\n",
        "        print(f\"Collected {articles_collected}/{max_articles} articles.\")\n",
        "\n",
        "collect_articles(max_articles=1000)\n",
        "\n",
        "df = pd.DataFrame(articles, columns=['Title', 'Venue', 'Year', 'Authors', 'Abstract'])\n",
        "df.to_csv('google_scholar_XYZ_articles.csv', index=False)\n",
        "\n",
        "print(\"Data collection completed. Articles saved in 'google_scholar_XYZ_articles.csv'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b57bdb5-ce8d-40c6-c282-4a5ea466155b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: asyncpraw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: aiofiles<1 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.8.0)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (3.10.5)\n",
            "Requirement already satisfied: aiosqlite<=0.17.0 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.17.0)\n",
            "Requirement already satisfied: asyncprawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.18.0)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (4.0.3)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.18->asyncpraw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2024.8.30)\n",
            "Reddit data saved as 'reddit_posts_async.csv'\n"
          ]
        }
      ],
      "source": [
        "!pip install nest_asyncio asyncpraw pandas\n",
        "\n",
        "import asyncpraw\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def collect_reddit_data(subreddit_name, search_keyword, max_posts=100):\n",
        "    async with asyncpraw.Reddit(\n",
        "       client_id=\"RqGydbViVA8Zg_Dx9L7T-Q\",\n",
        "    client_secret=\"5tmoqNGqH2cJ8fGB29jevGsF7LJBAQ\",\n",
        "    user_agent=\"u/Alarmed-Rub87\"\n",
        "\n",
        "    ) as reddit:\n",
        "\n",
        "        subreddit = await reddit.subreddit(subreddit_name)\n",
        "        posts = []\n",
        "\n",
        "        async for submission in subreddit.search(search_keyword, limit=max_posts):\n",
        "            post_data = {\n",
        "                'Title': submission.title,\n",
        "                'Author': submission.author.name if submission.author else 'N/A',\n",
        "                'Score': submission.score,\n",
        "                'Number of Comments': submission.num_comments,\n",
        "                'Date': submission.created_utc,\n",
        "                'URL': submission.url,\n",
        "                'Flair': submission.link_flair_text,\n",
        "            }\n",
        "            posts.append(post_data)\n",
        "\n",
        "        return pd.DataFrame(posts)\n",
        "\n",
        "async def main():\n",
        "    subreddit = \"technology\"\n",
        "    keyword = \"AI\"\n",
        "    df = await collect_reddit_data(subreddit, keyword, max_posts=200)\n",
        "\n",
        "    df.to_csv(\"reddit_ai_posts_async.csv\", index=False)\n",
        "    print(\"Reddit data saved as 'reddit_posts_async.csv'\")\n",
        "\n",
        "await main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "\n",
        "Learning Experience: Although many pages can be scraped with simple requests and parsing tools, I discovered that some websites use JavaScript to load content dynamically.\n",
        "In these situations, tools like Selenium or Async PRAW APIs come in handy because they can deal with asynchronous requests and JavaScript.\n",
        "\n",
        "Challenges Encountered:I ran into rate limits when working with Reddit and Twitter (X) APIs, which limit how many API requests you can submit in a given amount of time.\n",
        "This had an impact on data collection efficiency, especially when trying to collect a lot of data.\n",
        "\n",
        "Relevance to Your Field of Study: Large-scale, current datasets are essential for many fields, including business analytics, social sciences, machine learning, and data science.\n",
        "I can gather up-to-date information from websites like Reddit, Twitter, and news sources by using web scraping.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e9ac8d12-4188-4cb1-d110-cae17162ba20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWrite your response here.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}