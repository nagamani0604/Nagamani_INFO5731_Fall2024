{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagamani0604/Nagamani_INFO5731_Fall2024/blob/main/Somireddy_Nagamani_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "\n",
        "How can adults in high-pressure jobs reduce stress and improve the quality of their sleep with regular mindfulness meditation?\n",
        "\n",
        "Data to be collected:\n",
        "1. Information on Mindfulness Meditation:\n",
        "Duration,consistency,type and time : Lenght of the time, frequency of practice, What type of meditation at what time during the day is the basic data that needs to be collected\n",
        "2. Sleep quality: Sleep duration and efficiency\n",
        "3. Stress Level: Cortisol levels and heart rate variability\n",
        "4. Demographic data: Age, gender, job type, job role and work hours\n",
        "\n",
        "Amount of data needed for analysis:\n",
        "1. There should be atleast 200 participants from high pressure jobs and even more for reliable results\n",
        "2. All the particiapnts should follow mindfulness meditation atleast for 4-5 weeks to observe any changes in the stress levels and sleep patterns\n",
        "3. Stress levels and sleep quality should be measured before starting the assessment, mid-assessment and at the end of the study.\n",
        "\n",
        "Steps for collecting and saving the data:\n",
        "1. We need to recruit participants who are working in high stressful jobs like IT employees or health care workers who are interested in mindfulness meditation\n",
        "2. All the participants should have access to the mindfulness medidation apps. We need to ensure the participants log in into their apps and track the duration and frequency\n",
        "3. Track the sleep patterns like total sleep time, efficiency and awakenings\n",
        "4. Measure the stress levels by instructing the participants to wear devices that track heart rate variability\n",
        "5. Save all the collected data in a database.\n",
        "6. Compare pre and post meditation data and analyse the improvements in sleep quality and stress reduction.\n",
        "7. Continuosly follow up and montor the participants medidation schedule to ensure the accurate data is collected.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "def generate_duration():\n",
        "    return random.choice([20, 40, 60])\n",
        "\n",
        "def generate_sleep_quality():\n",
        "    return random.randint(10, 10)\n",
        "\n",
        "def generate_stress_level():\n",
        "    return random.randint(1, 10)\n",
        "\n",
        "data = []\n",
        "for i in range(1000):\n",
        "    duration = generate_duration()\n",
        "    sleep_quality = generate_sleep_quality()\n",
        "    stress_level = generate_stress_level()\n",
        "\n",
        "    data.append([duration, sleep_quality, stress_level])\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"Duration\", \"Sleep_Quality\", \"Stress_Level\"])\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "df.to_csv(\"simple_meditation_dataset.csv\", index=False)\n",
        "\n",
        "print(\"Dataset generated and saved as 'simple_meditation_dataset.csv'.\")\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43823e2-f775-47fa-c1f4-2629aac35464"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Duration  Sleep_Quality  Stress_Level\n",
            "0        20             10             6\n",
            "1        40             10             2\n",
            "2        60             10             6\n",
            "3        40             10             2\n",
            "4        40             10             6\n",
            "Dataset generated and saved as 'simple_meditation_dataset.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046a0386-91ab-4ad7-bfdc-edcbcdfcf653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 100/1000 articles.\n",
            "Collected 200/1000 articles.\n",
            "Collected 300/1000 articles.\n",
            "Collected 400/1000 articles.\n",
            "Collected 500/1000 articles.\n",
            "Collected 600/1000 articles.\n",
            "Collected 700/1000 articles.\n",
            "Collected 800/1000 articles.\n",
            "Collected 900/1000 articles.\n",
            "Collected 1000/1000 articles.\n",
            "Data collection completed. Articles are saved in 'crossref_XYZ_articles.csv'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "BASE_URL = 'https://api.crossref.org/works'\n",
        "\n",
        "query = \"XYZ\"\n",
        "start_year = 2014\n",
        "end_year = 2024\n",
        "total_articles = 1000\n",
        "\n",
        "articles = []\n",
        "\n",
        "rows_per_request = 100\n",
        "offset = 0\n",
        "\n",
        "while len(articles) < total_articles:\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'filter': f'from-pub-date:{start_year}-01-01,until-pub-date:{end_year}-12-31',\n",
        "        'rows': rows_per_request,\n",
        "        'offset': offset\n",
        "    }\n",
        "\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json().get('message', {}).get('items', [])\n",
        "\n",
        "        for item in data:\n",
        "            title = item.get('title', ['N/A'])[0]\n",
        "            venue = item.get('container-title', ['N/A'])[0]  # Journal or conference\n",
        "            year = item.get('published-print', {}).get('date-parts', [[None]])[0][0] or 'N/A'\n",
        "            authors = ', '.join([author.get('given', '') + ' ' + author.get('family', '') for author in item.get('author', [])])\n",
        "            abstract = item.get('abstract', 'N/A')\n",
        "\n",
        "            articles.append([title, venue, year, authors, abstract])\n",
        "\n",
        "        offset += rows_per_request\n",
        "\n",
        "        print(f\"Collected {len(articles)}/{total_articles} articles.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        break\n",
        "\n",
        "\n",
        "df = pd.DataFrame(articles, columns=['Title', 'Venue', 'Year', 'Authors', 'Abstract'])\n",
        "df.to_csv(\"crossref_XYZ_articles.csv\", index=False)\n",
        "\n",
        "print(\"Data collection completed. Articles are saved in 'crossref_XYZ_articles.csv'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75beb849-00b0-4f0f-fe03-10b9b730ebbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Reddit data saved as 'reddit_ai_posts_async.csv'\n"
          ]
        }
      ],
      "source": [
        "!pip install nest_asyncio\n",
        "\n",
        "import asyncpraw\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def collect_reddit_data(subreddit_name, search_keyword, max_posts=100):\n",
        "    async with asyncpraw.Reddit(\n",
        "       client_id=\"RqGydbViVA8Zg_Dx9L7T-Q\",\n",
        "    client_secret=\"5tmoqNGqH2cJ8fGB29jevGsF7LJBAQ\",\n",
        "    user_agent=\"u/Alarmed-Rub87\"\n",
        "\n",
        "    ) as reddit:\n",
        "\n",
        "        subreddit = await reddit.subreddit(subreddit_name)\n",
        "        posts = []\n",
        "\n",
        "        async for submission in subreddit.search(search_keyword, limit=max_posts):\n",
        "            post_data = {\n",
        "                'Title': submission.title,\n",
        "                'Author': submission.author.name if submission.author else 'N/A',\n",
        "                'Score': submission.score,\n",
        "                'Number of Comments': submission.num_comments,\n",
        "                'Date': submission.created_utc,\n",
        "                'URL': submission.url,\n",
        "                'Flair': submission.link_flair_text,\n",
        "            }\n",
        "            posts.append(post_data)\n",
        "\n",
        "        return pd.DataFrame(posts)\n",
        "\n",
        "async def main():\n",
        "    subreddit = \"technology\"\n",
        "    keyword = \"AI\"\n",
        "    df = await collect_reddit_data(subreddit, keyword, max_posts=200)\n",
        "\n",
        "    df.to_csv(\"reddit_ai_posts_async.csv\", index=False)\n",
        "    print(\"Reddit data saved as 'reddit_ai_posts_async.csv'\")\n",
        "\n",
        "await main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "\n",
        "Learning Experience: Although many pages can be scraped with simple requests and parsing tools, I discovered that some websites use JavaScript to load content dynamically.\n",
        "In these situations, tools like Selenium or Async PRAW APIs come in handy because they can deal with asynchronous requests and JavaScript.\n",
        "\n",
        "Challenges Encountered:I ran into rate limits when working with Reddit and Twitter (X) APIs, which limit how many API requests you can submit in a given amount of time.\n",
        "This had an impact on data collection efficiency, especially when trying to collect a lot of data.\n",
        "\n",
        "Relevance to Your Field of Study: Large-scale, current datasets are essential for many fields, including business analytics, social sciences, machine learning, and data science.\n",
        "I can gather up-to-date information from websites like Reddit, Twitter, and news sources by using web scraping.\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}