{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagamani0604/Nagamani_INFO5731_Fall2024/blob/main/Somireddy_Nagamani_Assignment_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "685fdf88-94b8-478c-e96d-8ad332dc47cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Fetching 10000 more abstracts for query: 'machine learning'\n",
            "Fetching 9900 more abstracts for query: 'machine learning'\n",
            "Fetching 9800 more abstracts for query: 'machine learning'\n",
            "Fetching 9700 more abstracts for query: 'machine learning'\n",
            "Fetching 9600 more abstracts for query: 'machine learning'\n",
            "Fetching 9500 more abstracts for query: 'machine learning'\n",
            "Fetching 9400 more abstracts for query: 'machine learning'\n",
            "Fetching 9300 more abstracts for query: 'machine learning'\n",
            "Fetching 9200 more abstracts for query: 'machine learning'\n",
            "Fetching 9100 more abstracts for query: 'machine learning'\n",
            "Fetching 9000 more abstracts for query: 'machine learning'\n",
            "Error fetching data: 400\n",
            "Fetching 9000 more abstracts for query: 'machine learning'\n",
            "Fetching 8900 more abstracts for query: 'machine learning'\n",
            "Fetching 8800 more abstracts for query: 'machine learning'\n",
            "Fetching 8700 more abstracts for query: 'machine learning'\n",
            "Fetching 8600 more abstracts for query: 'machine learning'\n",
            "Fetching 8500 more abstracts for query: 'machine learning'\n",
            "Fetching 8400 more abstracts for query: 'machine learning'\n",
            "Fetching 8300 more abstracts for query: 'machine learning'\n",
            "Fetching 8200 more abstracts for query: 'machine learning'\n",
            "Fetching 8100 more abstracts for query: 'machine learning'\n",
            "Fetching 8000 more abstracts for query: 'machine learning'\n",
            "Error fetching data: 400\n",
            "Fetching 8000 more abstracts for query: 'data science'\n",
            "Fetching 7900 more abstracts for query: 'data science'\n",
            "Fetching 7800 more abstracts for query: 'data science'\n",
            "Fetching 7700 more abstracts for query: 'data science'\n",
            "Fetching 7600 more abstracts for query: 'data science'\n",
            "Fetching 7500 more abstracts for query: 'data science'\n",
            "Fetching 7400 more abstracts for query: 'data science'\n",
            "Fetching 7300 more abstracts for query: 'data science'\n",
            "Fetching 7200 more abstracts for query: 'data science'\n",
            "Fetching 7100 more abstracts for query: 'data science'\n",
            "Fetching 7000 more abstracts for query: 'data science'\n",
            "Error fetching data: 400\n",
            "Fetching 7000 more abstracts for query: 'data science'\n",
            "Fetching 6900 more abstracts for query: 'data science'\n",
            "Fetching 6800 more abstracts for query: 'data science'\n",
            "Fetching 6700 more abstracts for query: 'data science'\n",
            "Fetching 6600 more abstracts for query: 'data science'\n",
            "Fetching 6500 more abstracts for query: 'data science'\n",
            "Fetching 6400 more abstracts for query: 'data science'\n",
            "Fetching 6300 more abstracts for query: 'data science'\n",
            "Fetching 6200 more abstracts for query: 'data science'\n",
            "Fetching 6100 more abstracts for query: 'data science'\n",
            "Fetching 6000 more abstracts for query: 'data science'\n",
            "Error fetching data: 400\n",
            "Fetching 6000 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 5900 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 5800 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 5700 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 5600 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 5500 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 5400 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 5300 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 5200 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 5100 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 5000 more abstracts for query: 'artificial intelligence'\n",
            "Error fetching data: 400\n",
            "Fetching 5000 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 4900 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 4800 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 4700 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 4600 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 4500 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 4400 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 4300 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 4200 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 4100 more abstracts for query: 'artificial intelligence'\n",
            "Fetching 4000 more abstracts for query: 'artificial intelligence'\n",
            "Error fetching data: 400\n",
            "Fetching 4000 more abstracts for query: 'information extraction'\n",
            "Fetching 3900 more abstracts for query: 'information extraction'\n",
            "Fetching 3800 more abstracts for query: 'information extraction'\n",
            "Fetching 3700 more abstracts for query: 'information extraction'\n",
            "Fetching 3600 more abstracts for query: 'information extraction'\n",
            "Fetching 3500 more abstracts for query: 'information extraction'\n",
            "Fetching 3400 more abstracts for query: 'information extraction'\n",
            "Fetching 3300 more abstracts for query: 'information extraction'\n",
            "Fetching 3200 more abstracts for query: 'information extraction'\n",
            "Fetching 3100 more abstracts for query: 'information extraction'\n",
            "Fetching 3000 more abstracts for query: 'information extraction'\n",
            "Error fetching data: 400\n",
            "Fetching 3000 more abstracts for query: 'information extraction'\n",
            "Fetching 2900 more abstracts for query: 'information extraction'\n",
            "Fetching 2800 more abstracts for query: 'information extraction'\n",
            "Fetching 2700 more abstracts for query: 'information extraction'\n",
            "Fetching 2600 more abstracts for query: 'information extraction'\n",
            "Fetching 2500 more abstracts for query: 'information extraction'\n",
            "Fetching 2400 more abstracts for query: 'information extraction'\n",
            "Fetching 2300 more abstracts for query: 'information extraction'\n",
            "Fetching 2200 more abstracts for query: 'information extraction'\n",
            "Fetching 2100 more abstracts for query: 'information extraction'\n",
            "Fetching 2000 more abstracts for query: 'information extraction'\n",
            "Error fetching data: 400\n",
            "Fetching 2000 more abstracts for query: 'deep learning'\n",
            "Fetching 1900 more abstracts for query: 'deep learning'\n",
            "Fetching 1800 more abstracts for query: 'deep learning'\n",
            "Fetching 1700 more abstracts for query: 'deep learning'\n",
            "Fetching 1600 more abstracts for query: 'deep learning'\n",
            "Fetching 1500 more abstracts for query: 'deep learning'\n",
            "Fetching 1400 more abstracts for query: 'deep learning'\n",
            "Fetching 1300 more abstracts for query: 'deep learning'\n",
            "Fetching 1200 more abstracts for query: 'deep learning'\n",
            "Fetching 1100 more abstracts for query: 'deep learning'\n",
            "Fetching 1000 more abstracts for query: 'deep learning'\n",
            "Error fetching data: 400\n",
            "Fetching 1000 more abstracts for query: 'deep learning'\n",
            "Fetching 900 more abstracts for query: 'deep learning'\n",
            "Fetching 800 more abstracts for query: 'deep learning'\n",
            "Fetching 700 more abstracts for query: 'deep learning'\n",
            "Fetching 600 more abstracts for query: 'deep learning'\n",
            "Fetching 500 more abstracts for query: 'deep learning'\n",
            "Fetching 400 more abstracts for query: 'deep learning'\n",
            "Fetching 300 more abstracts for query: 'deep learning'\n",
            "Fetching 200 more abstracts for query: 'deep learning'\n",
            "Fetching 100 more abstracts for query: 'deep learning'\n",
            "Saved 10000 abstracts to papers_abstracts.csv.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install requests pandas\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import urllib.parse\n",
        "\n",
        "API_KEY = 'ov4H66zruU14lwrB02ZPo1AqupejrEPZ6lRFSRps'\n",
        "\n",
        "def fetch_paper_data(query, offset=0, limit=100):\n",
        "    \"\"\"\n",
        "    Fetch paper data from the Semantic Scholar API.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query.\n",
        "        offset (int): The offset for pagination.\n",
        "        limit (int): The number of records to retrieve per request.\n",
        "\n",
        "    Returns:\n",
        "        dict: The JSON response from the API.\n",
        "    \"\"\"\n",
        "    encoded_query = urllib.parse.quote(query)\n",
        "\n",
        "    url = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={encoded_query}&fields=title,abstract&offset={offset}&limit={limit}\"\n",
        "\n",
        "    headers = {\n",
        "        'x-api-key': API_KEY\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error fetching data: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "    return response.json()\n",
        "\n",
        "def collect_abstracts(queries, total_abstracts=10000):\n",
        "    \"\"\"\n",
        "    Collect abstracts from Semantic Scholar based on given queries.\n",
        "\n",
        "    Args:\n",
        "        queries (list): A list of search queries.\n",
        "        total_abstracts (int): The total number of abstracts to collect.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing the title and abstract of papers.\n",
        "    \"\"\"\n",
        "    all_abstracts = []\n",
        "\n",
        "    for query in queries:\n",
        "        offset = 0\n",
        "        while len(all_abstracts) < total_abstracts:\n",
        "            print(f\"Fetching {total_abstracts - len(all_abstracts)} more abstracts for query: '{query}'\")\n",
        "            data = fetch_paper_data(query, offset)\n",
        "\n",
        "            if data is None or 'data' not in data:\n",
        "                break\n",
        "\n",
        "            for paper in data['data']:\n",
        "                if 'abstract' in paper:\n",
        "                    all_abstracts.append({\n",
        "                        'Title': paper['title'],\n",
        "                        'Abstract': paper['abstract']\n",
        "                    })\n",
        "\n",
        "            offset += len(data['data'])\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "        if len(all_abstracts) >= total_abstracts:\n",
        "            break\n",
        "\n",
        "    return all_abstracts[:total_abstracts]\n",
        "\n",
        "def save_to_csv(data, filename='papers_abstracts.csv'):\n",
        "    \"\"\"\n",
        "    Save the collected data to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        data (list): The data to save.\n",
        "        filename (str): The name of the output CSV file.\n",
        "    \"\"\"\n",
        "    if not data:\n",
        "        print(\"No data to save.\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saved {len(data)} abstracts to {filename}.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    queries = [\n",
        "        \"machine learning\",\n",
        "          \"machine learning\",\n",
        "        \"data science\",\n",
        "                \"data science\",\n",
        "        \"artificial intelligence\",\n",
        "        \"artificial intelligence\",\n",
        "        \"information extraction\",\n",
        "                \"information extraction\",\n",
        "        \"deep learning\",\n",
        "        \"deep learning\",\n",
        "    ]\n",
        "\n",
        "    abstracts_info = collect_abstracts(queries, total_abstracts=10000)\n",
        "    save_to_csv(abstracts_info)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deda2a01-e5f7-4b37-999a-cdaed3dfa365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Abstract  \\\n",
            "0  We present Fashion-MNIST, a new dataset compri...   \n",
            "1  TensorFlow is a machine learning system that o...   \n",
            "2  TensorFlow is an interface for expressing mach...   \n",
            "3                                                NaN   \n",
            "4  The goal of precipitation nowcasting is to pre...   \n",
            "\n",
            "                                    Cleaned_Abstract  \\\n",
            "0  present fashionmnist new dataset comprising x ...   \n",
            "1  tensorflow machine learning system operates la...   \n",
            "2  tensorflow interface expressing machine learni...   \n",
            "3                                                      \n",
            "4  goal precipitation nowcasting predict future r...   \n",
            "\n",
            "                                    Stemmed_Abstract  \\\n",
            "0  present fashionmnist new dataset compris x gra...   \n",
            "1  tensorflow machin learn system oper larg scale...   \n",
            "2  tensorflow interfac express machin learn algor...   \n",
            "3                                                      \n",
            "4  goal precipit nowcast predict futur rainfal in...   \n",
            "\n",
            "                                 Lemmatized_Abstract  \n",
            "0  present fashionmnist new dataset comprising x ...  \n",
            "1  tensorflow machine learning system operates la...  \n",
            "2  tensorflow interface expressing machine learni...  \n",
            "3                                                     \n",
            "4  goal precipitation nowcasting predict future r...  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install pandas nltk\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "df = pd.read_csv('papers_abstracts.csv')\n",
        "\n",
        "if 'Abstract' not in df.columns:\n",
        "    raise ValueError(\"The 'Abstract' column is not found in the CSV file.\")\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def to_string(text):\n",
        "    if isinstance(text, str):\n",
        "        return text\n",
        "    return ''\n",
        "\n",
        "def remove_noise(text):\n",
        "    text = to_string(text)\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "def remove_numbers(text):\n",
        "    text = to_string(text)\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    text = to_string(text)\n",
        "    words = text.split()\n",
        "    return ' '.join([word for word in words if word.lower() not in stop_words])\n",
        "\n",
        "def to_lowercase(text):\n",
        "    text = to_string(text)\n",
        "    return text.lower()\n",
        "\n",
        "def stem_text(text):\n",
        "    text = to_string(text)\n",
        "    words = text.split()\n",
        "    return ' '.join([ps.stem(word) for word in words])\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    text = to_string(text)\n",
        "    words = text.split()\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "df['Cleaned_Abstract'] = df['Abstract'].apply(remove_noise)\n",
        "df['Cleaned_Abstract'] = df['Cleaned_Abstract'].apply(remove_numbers)\n",
        "df['Cleaned_Abstract'] = df['Cleaned_Abstract'].apply(remove_stopwords)\n",
        "df['Cleaned_Abstract'] = df['Cleaned_Abstract'].apply(to_lowercase)\n",
        "\n",
        "df['Stemmed_Abstract'] = df['Cleaned_Abstract'].apply(stem_text)\n",
        "df['Lemmatized_Abstract'] = df['Cleaned_Abstract'].apply(lemmatize_text)\n",
        "\n",
        "df.to_csv('cleaned_papers_abstracts.csv', index=False)\n",
        "\n",
        "print(df[['Abstract', 'Cleaned_Abstract', 'Stemmed_Abstract', 'Lemmatized_Abstract']].head())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "065f6737-98e2-45c6-83c6-1007cb15a9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parts of Speech Counts:\n",
            "Nouns         379338\n",
            "Verbs         153280\n",
            "Adjectives    163794\n",
            "Adverbs        34678\n",
            "dtype: int64\n",
            "\n",
            "Constituency Parsing Example (First Abstract):\n",
            "(S\n",
            "  present/JJ\n",
            "  fashionmnist/JJ\n",
            "  new/JJ\n",
            "  dataset/NN\n",
            "  comprising/VBG\n",
            "  x/JJ\n",
            "  grayscale/JJ\n",
            "  images/NNS\n",
            "  fashion/NN\n",
            "  products/NNS\n",
            "  categories/NNS\n",
            "  images/NNS\n",
            "  per/IN\n",
            "  category/NN\n",
            "  training/NN\n",
            "  set/VBN\n",
            "  images/NNS\n",
            "  test/VBP\n",
            "  set/VBN\n",
            "  images/NNS\n",
            "  fashionmnist/VBP\n",
            "  intended/VBN\n",
            "  serve/VBP\n",
            "  direct/JJ\n",
            "  dropin/NN\n",
            "  replacement/NN\n",
            "  original/JJ\n",
            "  mnist/NN\n",
            "  dataset/NN\n",
            "  benchmarking/NN\n",
            "  machine/NN\n",
            "  learning/VBG\n",
            "  algorithms/JJ\n",
            "  shares/NNS\n",
            "  image/NN\n",
            "  size/NN\n",
            "  data/NNS\n",
            "  format/NN\n",
            "  structure/NN\n",
            "  training/VBG\n",
            "  testing/VBG\n",
            "  splits/NNS\n",
            "  dataset/VBN\n",
            "  freely/RB\n",
            "  available/JJ\n",
            "  https/NN\n",
            "  url/NN)\n",
            "\n",
            "Named Entity Recognition Results (First Abstract):\n",
            "(S\n",
            "  present/JJ\n",
            "  fashionmnist/JJ\n",
            "  new/JJ\n",
            "  dataset/NN\n",
            "  comprising/VBG\n",
            "  x/JJ\n",
            "  grayscale/JJ\n",
            "  images/NNS\n",
            "  fashion/NN\n",
            "  products/NNS\n",
            "  categories/NNS\n",
            "  images/NNS\n",
            "  per/IN\n",
            "  category/NN\n",
            "  training/NN\n",
            "  set/VBN\n",
            "  images/NNS\n",
            "  test/VBP\n",
            "  set/VBN\n",
            "  images/NNS\n",
            "  fashionmnist/VBP\n",
            "  intended/VBN\n",
            "  serve/VBP\n",
            "  direct/JJ\n",
            "  dropin/NN\n",
            "  replacement/NN\n",
            "  original/JJ\n",
            "  mnist/NN\n",
            "  dataset/NN\n",
            "  benchmarking/NN\n",
            "  machine/NN\n",
            "  learning/VBG\n",
            "  algorithms/JJ\n",
            "  shares/NNS\n",
            "  image/NN\n",
            "  size/NN\n",
            "  data/NNS\n",
            "  format/NN\n",
            "  structure/NN\n",
            "  training/VBG\n",
            "  testing/VBG\n",
            "  splits/NNS\n",
            "  dataset/VBN\n",
            "  freely/RB\n",
            "  available/JJ\n",
            "  https/NN\n",
            "  url/NN)\n",
            "                                                                                                                                                                                                                                                                     S                                                                                                                                                                                                                                                                      \n",
            "     ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________|__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________     \n",
            "present/JJ fashionmnist/JJ new/JJ dataset/NN comprising/VBG x/JJ grayscale/JJ images/NNS fashion/NN products/NNS categories/NNS images/NNS per/IN category/NN training/NN set/VBN images/NNS test/VBP set/VBN images/NNS fashionmnist/VBP intended/VBN serve/VBP direct/JJ dropin/NN replacement/NN original/JJ mnist/NN dataset/NN benchmarking/NN machine/NN learning/VBG algorithms/JJ shares/NNS image/NN size/NN data/NNS format/NN structure/NN training/VBG testing/VBG splits/NNS dataset/VBN freely/RB available/JJ https/NN url/NN\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas nltk spacy\n",
        "!pip install nltk\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize, ne_chunk\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "df = pd.read_csv('cleaned_papers_abstracts.csv')\n",
        "\n",
        "df['Cleaned_Abstract'] = df['Cleaned_Abstract'].astype(str)\n",
        "\n",
        "def pos_tagging(text):\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags = pos_tag(words)\n",
        "    return pos_tags\n",
        "\n",
        "def count_pos(pos_tags):\n",
        "    counts = Counter(tag for word, tag in pos_tags)\n",
        "    return {\n",
        "        'Nouns': counts['NN'] + counts['NNS'] + counts['NNP'] + counts['NNPS'],\n",
        "        'Verbs': counts['VB'] + counts['VBD'] + counts['VBG'] + counts['VBN'] + counts['VBP'] + counts['VBZ'],\n",
        "        'Adjectives': counts['JJ'] + counts['JJR'] + counts['JJS'],\n",
        "        'Adverbs': counts['RB'] + counts['RBR'] + counts['RBS']\n",
        "    }\n",
        "\n",
        "def parse_sentences(text):\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags = pos_tag(words)\n",
        "    chunks = ne_chunk(pos_tags)\n",
        "    return chunks\n",
        "\n",
        "def named_entity_recognition(text):\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags = pos_tag(words)\n",
        "    named_entities = ne_chunk(pos_tags)\n",
        "    return named_entities\n",
        "\n",
        "pos_counts = []\n",
        "constituency_parsing = []\n",
        "ner_results = []\n",
        "\n",
        "for abstract in df['Cleaned_Abstract']:\n",
        "    pos_tags = pos_tagging(abstract)\n",
        "    pos_counts.append(count_pos(pos_tags))\n",
        "\n",
        "    constituency_tree = parse_sentences(abstract)\n",
        "    constituency_parsing.append(constituency_tree)\n",
        "\n",
        "    ner_entities = named_entity_recognition(abstract)\n",
        "    ner_results.append(ner_entities)\n",
        "\n",
        "pos_counts_df = pd.DataFrame(pos_counts)\n",
        "df = pd.concat([df, pos_counts_df], axis=1)\n",
        "\n",
        "print(\"Parts of Speech Counts:\")\n",
        "print(df[['Nouns', 'Verbs', 'Adjectives', 'Adverbs']].sum())\n",
        "print(\"\\nConstituency Parsing Example (First Abstract):\")\n",
        "print(constituency_parsing[0])\n",
        "print(\"\\nNamed Entity Recognition Results (First Abstract):\")\n",
        "print(ner_results[0])\n",
        "\n",
        "df.to_csv('syntax_structure_analysis.csv', index=False)\n",
        "\n",
        "def visualize_tree(tree):\n",
        "    tree.pretty_print()\n",
        "\n",
        "if len(constituency_parsing) > 0:\n",
        "    visualize_tree(constituency_parsing[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "\n",
        "There are difficulties associated with handling API authentication, data quotas, and rate limitation when using external APIs like Semantic Scholar.\n",
        "The complexity of troubleshooting issues such as 400 errors resulting from query formatting or request limits increases.\n",
        "\n",
        "It's very powerful to use Python to automate data collection and manipulation, from web scraping to storing the results in CSV files.\n",
        "It shows how much you can do in comparatively short amounts of time with programming.\n",
        "\n",
        "Ten to fifteen days would be a reasonable time to complete an assignment like this, allowing for detailed testing, research into API documentation, and implementing solutions for any unexpected issues that arise.\n"
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}